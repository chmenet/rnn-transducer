[2019-09-18 07:43:27,812 INFO] Save config info.
[2019-09-18 07:43:28,150 INFO] Load Train Set!
[2019-09-18 07:43:28,239 INFO] Load Dev Set!
[2019-09-18 07:43:28,240 INFO] Set random seed: 2019
[2019-09-18 07:43:35,374 INFO] Loaded the model to 1 GPUs
[2019-09-18 07:43:35,374 INFO] # the number of parameters in the whole model: 1851797
[2019-09-18 07:43:35,375 INFO] # the number of parameters in the Encoder: 1202304
[2019-09-18 07:43:35,375 INFO] # the number of parameters in the Decoder: 597376
[2019-09-18 07:43:35,375 INFO] # the number of parameters in the JointNet: 52117
[2019-09-18 07:43:35,375 INFO] Created a sgd optimizer.
[2019-09-18 07:43:35,386 INFO] Created a visualizer.
[2019-09-18 07:44:28,539 INFO] Save config info.
[2019-09-18 07:44:28,781 INFO] Load Train Set!
[2019-09-18 07:44:28,874 INFO] Load Dev Set!
[2019-09-18 07:44:28,875 INFO] Set random seed: 2019
[2019-09-18 07:44:36,577 INFO] Loaded the model to 1 GPUs
[2019-09-18 07:44:36,577 INFO] # the number of parameters in the whole model: 1851797
[2019-09-18 07:44:36,578 INFO] # the number of parameters in the Encoder: 1202304
[2019-09-18 07:44:36,578 INFO] # the number of parameters in the Decoder: 597376
[2019-09-18 07:44:36,578 INFO] # the number of parameters in the JointNet: 52117
[2019-09-18 07:44:36,578 INFO] Created a sgd optimizer.
[2019-09-18 07:44:36,587 INFO] Created a visualizer.
[2019-09-18 07:48:04,541 INFO] Save config info.
[2019-09-18 07:48:04,811 INFO] Load Train Set!
[2019-09-18 07:48:04,989 INFO] Load Dev Set!
[2019-09-18 07:48:04,991 INFO] Set random seed: 2019
[2019-09-18 07:48:12,712 INFO] Loaded the model to 1 GPUs
[2019-09-18 07:48:12,712 INFO] # the number of parameters in the whole model: 1851797
[2019-09-18 07:48:12,712 INFO] # the number of parameters in the Encoder: 1202304
[2019-09-18 07:48:12,712 INFO] # the number of parameters in the Decoder: 597376
[2019-09-18 07:48:12,713 INFO] # the number of parameters in the JointNet: 52117
[2019-09-18 07:48:12,713 INFO] Created a sgd optimizer.
[2019-09-18 07:48:12,726 INFO] Created a visualizer.
[2019-09-18 07:48:45,491 INFO] Save config info.
[2019-09-18 07:48:45,774 INFO] Load Train Set!
[2019-09-18 07:48:45,945 INFO] Load Dev Set!
[2019-09-18 07:48:45,946 INFO] Set random seed: 2019
[2019-09-18 07:48:53,100 INFO] Loaded the model to 1 GPUs
[2019-09-18 07:48:53,101 INFO] # the number of parameters in the whole model: 1851797
[2019-09-18 07:48:53,101 INFO] # the number of parameters in the Encoder: 1202304
[2019-09-18 07:48:53,101 INFO] # the number of parameters in the Decoder: 597376
[2019-09-18 07:48:53,101 INFO] # the number of parameters in the JointNet: 52117
[2019-09-18 07:48:53,102 INFO] Created a sgd optimizer.
[2019-09-18 07:48:53,112 INFO] Created a visualizer.
[2019-09-18 07:49:03,561 INFO] Save config info.
[2019-09-18 07:49:03,687 INFO] Load Train Set!
[2019-09-18 07:49:03,742 INFO] Load Dev Set!
[2019-09-18 07:49:03,744 INFO] Set random seed: 2019
[2019-09-18 07:49:11,730 INFO] Loaded the model to 1 GPUs
[2019-09-18 07:49:11,730 INFO] # the number of parameters in the whole model: 1851797
[2019-09-18 07:49:11,730 INFO] # the number of parameters in the Encoder: 1202304
[2019-09-18 07:49:11,731 INFO] # the number of parameters in the Decoder: 597376
[2019-09-18 07:49:11,731 INFO] # the number of parameters in the JointNet: 52117
[2019-09-18 07:49:11,731 INFO] Created a sgd optimizer.
[2019-09-18 07:49:11,744 INFO] Created a visualizer.
[2019-09-18 07:51:06,919 INFO] Save config info.
[2019-09-18 07:51:07,153 INFO] Load Train Set!
[2019-09-18 07:51:07,313 INFO] Load Dev Set!
[2019-09-18 07:51:07,314 INFO] Set random seed: 2019
[2019-09-18 07:51:15,024 INFO] Loaded the model to 1 GPUs
[2019-09-18 07:51:15,024 INFO] # the number of parameters in the whole model: 1851797
[2019-09-18 07:51:15,024 INFO] # the number of parameters in the Encoder: 1202304
[2019-09-18 07:51:15,024 INFO] # the number of parameters in the Decoder: 597376
[2019-09-18 07:51:15,024 INFO] # the number of parameters in the JointNet: 52117
[2019-09-18 07:51:15,025 INFO] Created a sgd optimizer.
[2019-09-18 07:51:15,033 INFO] Created a visualizer.
[2019-09-18 07:52:49,088 INFO] Save config info.
[2019-09-18 07:52:49,217 INFO] Load Train Set!
[2019-09-18 07:52:49,271 INFO] Load Dev Set!
[2019-09-18 07:52:49,272 INFO] Set random seed: 2019
[2019-09-18 07:52:56,468 INFO] Loaded the model to 1 GPUs
[2019-09-18 07:52:56,468 INFO] # the number of parameters in the whole model: 1851797
[2019-09-18 07:52:56,468 INFO] # the number of parameters in the Encoder: 1202304
[2019-09-18 07:52:56,469 INFO] # the number of parameters in the Decoder: 597376
[2019-09-18 07:52:56,469 INFO] # the number of parameters in the JointNet: 52117
[2019-09-18 07:52:56,469 INFO] Created a sgd optimizer.
[2019-09-18 07:52:56,479 INFO] Created a visualizer.
[2019-09-18 07:56:13,208 INFO] Save config info.
[2019-09-18 07:56:13,335 INFO] Load Train Set!
[2019-09-18 07:56:13,386 INFO] Load Dev Set!
[2019-09-18 07:56:13,388 INFO] Set random seed: 2019
[2019-09-18 07:56:20,719 INFO] Loaded the model to 1 GPUs
[2019-09-18 07:56:20,720 INFO] # the number of parameters in the whole model: 1851797
[2019-09-18 07:56:20,720 INFO] # the number of parameters in the Encoder: 1202304
[2019-09-18 07:56:20,720 INFO] # the number of parameters in the Decoder: 597376
[2019-09-18 07:56:20,720 INFO] # the number of parameters in the JointNet: 52117
[2019-09-18 07:56:20,721 INFO] Created a sgd optimizer.
[2019-09-18 07:56:20,729 INFO] Created a visualizer.
[2019-09-18 08:03:48,912 INFO] Save config info.
[2019-09-18 08:03:49,154 INFO] Load Train Set!
[2019-09-18 08:03:49,275 INFO] Load Dev Set!
[2019-09-18 08:03:49,277 INFO] Set random seed: 2019
[2019-09-18 08:03:56,611 INFO] Loaded the model to 1 GPUs
[2019-09-18 08:03:56,611 INFO] # the number of parameters in the whole model: 1851797
[2019-09-18 08:03:56,611 INFO] # the number of parameters in the Encoder: 1202304
[2019-09-18 08:03:56,611 INFO] # the number of parameters in the Decoder: 597376
[2019-09-18 08:03:56,612 INFO] # the number of parameters in the JointNet: 52117
[2019-09-18 08:03:56,612 INFO] Created a sgd optimizer.
[2019-09-18 08:03:56,621 INFO] Created a visualizer.
[2019-09-18 08:05:26,368 INFO] Save config info.
[2019-09-18 08:05:26,560 INFO] Load Train Set!
[2019-09-18 08:05:26,621 INFO] Load Dev Set!
[2019-09-18 08:05:26,622 INFO] Set random seed: 2019
[2019-09-18 08:05:33,607 INFO] Loaded the model to 1 GPUs
[2019-09-18 08:05:33,608 INFO] # the number of parameters in the whole model: 1851797
[2019-09-18 08:05:33,608 INFO] # the number of parameters in the Encoder: 1202304
[2019-09-18 08:05:33,608 INFO] # the number of parameters in the Decoder: 597376
[2019-09-18 08:05:33,608 INFO] # the number of parameters in the JointNet: 52117
[2019-09-18 08:05:33,608 INFO] Created a sgd optimizer.
[2019-09-18 08:05:33,617 INFO] Created a visualizer.
[2019-09-18 08:16:38,977 INFO] Save config info.
[2019-09-18 08:16:39,077 INFO] Load Train Set!
[2019-09-18 08:16:39,131 INFO] Load Dev Set!
[2019-09-18 08:16:39,132 INFO] Set random seed: 2019
[2019-09-18 08:16:45,857 INFO] Loaded the model to 1 GPUs
[2019-09-18 08:16:45,858 INFO] # the number of parameters in the whole model: 1851797
[2019-09-18 08:16:45,858 INFO] # the number of parameters in the Encoder: 1202304
[2019-09-18 08:16:45,858 INFO] # the number of parameters in the Decoder: 597376
[2019-09-18 08:16:45,858 INFO] # the number of parameters in the JointNet: 52117
[2019-09-18 08:16:45,859 INFO] Created a sgd optimizer.
[2019-09-18 08:16:47,387 INFO] Created a visualizer.
[2019-09-18 08:17:36,656 INFO] Save config info.
[2019-09-18 08:17:36,786 INFO] Load Train Set!
[2019-09-18 08:17:36,838 INFO] Load Dev Set!
[2019-09-18 08:17:36,840 INFO] Set random seed: 2019
[2019-09-18 08:17:44,027 INFO] Loaded the model to 1 GPUs
[2019-09-18 08:17:44,027 INFO] # the number of parameters in the whole model: 1851797
[2019-09-18 08:17:44,027 INFO] # the number of parameters in the Encoder: 1202304
[2019-09-18 08:17:44,028 INFO] # the number of parameters in the Decoder: 597376
[2019-09-18 08:17:44,028 INFO] # the number of parameters in the JointNet: 52117
[2019-09-18 08:17:44,028 INFO] Created a sgd optimizer.
[2019-09-18 08:17:44,035 INFO] Created a visualizer.
[2019-09-18 08:18:09,571 INFO] Save config info.
[2019-09-18 08:18:09,721 INFO] Load Train Set!
[2019-09-18 08:18:09,776 INFO] Load Dev Set!
[2019-09-18 08:18:09,777 INFO] Set random seed: 2019
[2019-09-18 08:18:17,057 INFO] Loaded the model to 1 GPUs
[2019-09-18 08:18:17,058 INFO] # the number of parameters in the whole model: 1851797
[2019-09-18 08:18:17,058 INFO] # the number of parameters in the Encoder: 1202304
[2019-09-18 08:18:17,058 INFO] # the number of parameters in the Decoder: 597376
[2019-09-18 08:18:17,058 INFO] # the number of parameters in the JointNet: 52117
[2019-09-18 08:18:17,058 INFO] Created a sgd optimizer.
[2019-09-18 08:18:17,067 INFO] Created a visualizer.
[2019-09-18 08:26:04,064 INFO] Save config info.
[2019-09-18 08:26:04,337 INFO] Load Train Set!
[2019-09-18 08:26:04,390 INFO] Load Dev Set!
[2019-09-18 08:26:04,391 INFO] Set random seed: 2019
[2019-09-18 08:26:10,886 INFO] Loaded the model to 1 GPUs
[2019-09-18 08:26:10,887 INFO] # the number of parameters in the whole model: 1851797
[2019-09-18 08:26:10,887 INFO] # the number of parameters in the Encoder: 1202304
[2019-09-18 08:26:10,887 INFO] # the number of parameters in the Decoder: 597376
[2019-09-18 08:26:10,887 INFO] # the number of parameters in the JointNet: 52117
[2019-09-18 08:26:10,888 INFO] Created a sgd optimizer.
[2019-09-18 08:26:10,895 INFO] Created a visualizer.
[2019-09-18 08:26:41,522 INFO] Save config info.
[2019-09-18 08:26:41,759 INFO] Load Train Set!
[2019-09-18 08:26:41,923 INFO] Load Dev Set!
[2019-09-18 08:26:41,924 INFO] Set random seed: 2019
[2019-09-18 08:26:48,998 INFO] Loaded the model to 1 GPUs
[2019-09-18 08:26:48,999 INFO] # the number of parameters in the whole model: 1851797
[2019-09-18 08:26:48,999 INFO] # the number of parameters in the Encoder: 1202304
[2019-09-18 08:26:48,999 INFO] # the number of parameters in the Decoder: 597376
[2019-09-18 08:26:48,999 INFO] # the number of parameters in the JointNet: 52117
[2019-09-18 08:26:48,999 INFO] Created a sgd optimizer.
[2019-09-18 08:26:49,009 INFO] Created a visualizer.
[2019-09-18 08:29:35,490 INFO] Save config info.
[2019-09-18 08:29:35,735 INFO] Load Train Set!
[2019-09-18 08:29:35,785 INFO] Load Dev Set!
[2019-09-18 08:29:35,786 INFO] Set random seed: 2019
[2019-09-18 08:29:43,081 INFO] Loaded the model to 1 GPUs
[2019-09-18 08:29:43,082 INFO] # the number of parameters in the whole model: 1851797
[2019-09-18 08:29:43,082 INFO] # the number of parameters in the Encoder: 1202304
[2019-09-18 08:29:43,082 INFO] # the number of parameters in the Decoder: 597376
[2019-09-18 08:29:43,082 INFO] # the number of parameters in the JointNet: 52117
[2019-09-18 08:29:43,083 INFO] Created a sgd optimizer.
[2019-09-18 08:29:43,093 INFO] Created a visualizer.
[2019-09-18 08:32:40,425 INFO] Save config info.
[2019-09-18 08:32:40,519 INFO] Load Train Set!
[2019-09-18 08:32:40,565 INFO] Load Dev Set!
[2019-09-18 08:32:40,565 INFO] Set random seed: 2019
[2019-09-18 08:32:46,462 INFO] Loaded the model to 1 GPUs
[2019-09-18 08:32:46,462 INFO] # the number of parameters in the whole model: 1851797
[2019-09-18 08:32:46,462 INFO] # the number of parameters in the Encoder: 1202304
[2019-09-18 08:32:46,462 INFO] # the number of parameters in the Decoder: 597376
[2019-09-18 08:32:46,463 INFO] # the number of parameters in the JointNet: 52117
[2019-09-18 08:32:46,463 INFO] Created a sgd optimizer.
[2019-09-18 08:32:46,472 INFO] Created a visualizer.
[2019-09-18 08:34:45,982 INFO] Save config info.
[2019-09-18 08:34:46,225 INFO] Load Train Set!
[2019-09-18 08:34:46,455 INFO] Load Dev Set!
[2019-09-18 08:34:46,464 INFO] Set random seed: 2019
[2019-09-18 08:34:53,909 INFO] Loaded the model to 1 GPUs
[2019-09-18 08:34:53,911 INFO] # the number of parameters in the whole model: 1851797
[2019-09-18 08:34:53,912 INFO] # the number of parameters in the Encoder: 1202304
[2019-09-18 08:34:53,912 INFO] # the number of parameters in the Decoder: 597376
[2019-09-18 08:34:53,913 INFO] # the number of parameters in the JointNet: 52117
[2019-09-18 08:34:53,913 INFO] Created a sgd optimizer.
[2019-09-18 08:34:53,934 INFO] Created a visualizer.
[2019-09-18 08:36:41,663 INFO] Save config info.
[2019-09-18 08:36:41,798 INFO] Load Train Set!
[2019-09-18 08:36:41,851 INFO] Load Dev Set!
[2019-09-18 08:36:41,852 INFO] Set random seed: 2019
[2019-09-18 08:36:49,582 INFO] Loaded the model to 1 GPUs
[2019-09-18 08:36:49,583 INFO] # the number of parameters in the whole model: 1851797
[2019-09-18 08:36:49,583 INFO] # the number of parameters in the Encoder: 1202304
[2019-09-18 08:36:49,583 INFO] # the number of parameters in the Decoder: 597376
[2019-09-18 08:36:49,583 INFO] # the number of parameters in the JointNet: 52117
[2019-09-18 08:36:49,583 INFO] Created a sgd optimizer.
[2019-09-18 08:36:49,591 INFO] Created a visualizer.
[2019-09-18 08:37:01,055 INFO] Save config info.
[2019-09-18 08:37:01,304 INFO] Load Train Set!
[2019-09-18 08:37:01,470 INFO] Load Dev Set!
[2019-09-18 08:37:01,471 INFO] Set random seed: 2019
[2019-09-18 08:37:06,284 INFO] Save config info.
[2019-09-18 08:37:06,532 INFO] Load Train Set!
[2019-09-18 08:37:06,586 INFO] Load Dev Set!
[2019-09-18 08:37:06,587 INFO] Set random seed: 2019
[2019-09-18 08:37:13,823 INFO] Loaded the model to 1 GPUs
[2019-09-18 08:37:13,823 INFO] # the number of parameters in the whole model: 1851797
[2019-09-18 08:37:13,823 INFO] # the number of parameters in the Encoder: 1202304
[2019-09-18 08:37:13,823 INFO] # the number of parameters in the Decoder: 597376
[2019-09-18 08:37:13,824 INFO] # the number of parameters in the JointNet: 52117
[2019-09-18 08:37:13,824 INFO] Created a sgd optimizer.
[2019-09-18 08:37:13,833 INFO] Created a visualizer.
[2019-09-18 08:37:19,653 INFO] -Training-Epoch:0(0.13542%), Global Step:30, Learning Rate:0.000100, Grad Norm:725.43829, Loss:2734.24585, AverageLoss: 2322.89172, Run Time:0.205
[2019-09-18 08:37:21,642 INFO] -Training-Epoch:0(0.18212%), Global Step:40, Learning Rate:0.000100, Grad Norm:271.53735, Loss:687.63629, AverageLoss: 2308.64531, Run Time:0.067
[2019-09-18 08:37:23,719 INFO] -Training-Epoch:0(0.22881%), Global Step:50, Learning Rate:0.000100, Grad Norm:1926.59338, Loss:2321.33887, AverageLoss: 2269.48453, Run Time:0.218
[2019-09-18 08:37:25,520 INFO] -Training-Epoch:0(0.27551%), Global Step:60, Learning Rate:0.000100, Grad Norm:1690.17480, Loss:1146.96960, AverageLoss: 2121.28845, Run Time:0.147
[2019-09-18 08:37:45,716 INFO] Save config info.
[2019-09-18 08:37:45,927 INFO] Load Train Set!
[2019-09-18 08:37:46,098 INFO] Load Dev Set!
[2019-09-18 08:37:46,099 INFO] Set random seed: 2019
[2019-09-18 08:37:53,039 INFO] Loaded the model to 1 GPUs
[2019-09-18 08:37:53,040 INFO] # the number of parameters in the whole model: 1851797
[2019-09-18 08:37:53,041 INFO] # the number of parameters in the Encoder: 1202304
[2019-09-18 08:37:53,041 INFO] # the number of parameters in the Decoder: 597376
[2019-09-18 08:37:53,041 INFO] # the number of parameters in the JointNet: 52117
[2019-09-18 08:37:53,042 INFO] Created a sgd optimizer.
[2019-09-18 08:37:53,056 INFO] Created a visualizer.
[2019-09-18 08:38:30,622 INFO] Save config info.
[2019-09-18 08:38:30,787 INFO] Load Train Set!
[2019-09-18 08:38:30,951 INFO] Load Dev Set!
[2019-09-18 08:38:30,952 INFO] Set random seed: 2019
[2019-09-18 08:38:37,538 INFO] Loaded the model to 1 GPUs
[2019-09-18 08:38:37,540 INFO] # the number of parameters in the whole model: 1851797
[2019-09-18 08:38:37,541 INFO] # the number of parameters in the Encoder: 1202304
[2019-09-18 08:38:37,541 INFO] # the number of parameters in the Decoder: 597376
[2019-09-18 08:38:37,542 INFO] # the number of parameters in the JointNet: 52117
[2019-09-18 08:38:37,542 INFO] Created a sgd optimizer.
[2019-09-18 08:38:37,564 INFO] Created a visualizer.
[2019-09-18 08:39:02,126 INFO] -Training-Epoch:0(4.32836%), Global Step:30, Learning Rate:0.000100, Grad Norm:593.04181, Loss:2422.59497, AverageLoss: 2351.14996, Run Time:0.396
[2019-09-18 08:39:15,246 INFO] Save config info.
[2019-09-18 08:39:15,589 INFO] Load Train Set!
[2019-09-18 08:39:15,760 INFO] Load Dev Set!
[2019-09-18 08:39:15,762 INFO] Set random seed: 2019
[2019-09-18 08:39:22,768 INFO] Loaded the model to 1 GPUs
[2019-09-18 08:39:22,769 INFO] # the number of parameters in the whole model: 1851797
[2019-09-18 08:39:22,769 INFO] # the number of parameters in the Encoder: 1202304
[2019-09-18 08:39:22,769 INFO] # the number of parameters in the Decoder: 597376
[2019-09-18 08:39:22,769 INFO] # the number of parameters in the JointNet: 52117
[2019-09-18 08:39:22,770 INFO] Created a sgd optimizer.
[2019-09-18 08:39:22,778 INFO] Created a visualizer.
[2019-09-18 08:39:37,253 INFO] -Training-Epoch:0(2.16580%), Global Step:30, Learning Rate:0.000100, Grad Norm:586.76526, Loss:2336.79395, AverageLoss: 2385.53326, Run Time:0.336
[2019-09-18 08:39:41,611 INFO] -Training-Epoch:0(2.91262%), Global Step:40, Learning Rate:0.000100, Grad Norm:780.17267, Loss:1940.82629, AverageLoss: 2326.32720, Run Time:0.332
[2019-09-18 08:39:46,171 INFO] -Training-Epoch:0(3.65945%), Global Step:50, Learning Rate:0.000100, Grad Norm:1698.21863, Loss:1926.42651, AverageLoss: 2256.10402, Run Time:0.342
[2019-09-18 08:39:50,417 INFO] -Training-Epoch:0(4.40627%), Global Step:60, Learning Rate:0.000100, Grad Norm:2128.87793, Loss:1205.63391, AverageLoss: 2136.30772, Run Time:0.316
[2019-09-18 08:39:55,327 INFO] -Training-Epoch:0(5.15310%), Global Step:70, Learning Rate:0.000100, Grad Norm:1421.60754, Loss:920.99060, AverageLoss: 1976.65727, Run Time:0.333
[2019-09-18 08:39:59,652 INFO] -Training-Epoch:0(5.89993%), Global Step:80, Learning Rate:0.000100, Grad Norm:538.54919, Loss:688.41815, AverageLoss: 1821.59744, Run Time:0.317
[2019-09-18 08:40:04,175 INFO] -Training-Epoch:0(6.64675%), Global Step:90, Learning Rate:0.000100, Grad Norm:472.69009, Loss:737.66785, AverageLoss: 1698.14618, Run Time:0.328
[2019-09-18 08:40:08,590 INFO] -Training-Epoch:0(7.39358%), Global Step:100, Learning Rate:0.000100, Grad Norm:385.15030, Loss:612.76141, AverageLoss: 1592.63106, Run Time:0.341
[2019-09-18 08:40:13,225 INFO] -Training-Epoch:0(8.14040%), Global Step:110, Learning Rate:0.000100, Grad Norm:370.19797, Loss:654.60999, AverageLoss: 1503.38671, Run Time:0.342
[2019-09-18 08:40:17,720 INFO] -Training-Epoch:0(8.88723%), Global Step:120, Learning Rate:0.000100, Grad Norm:344.17780, Loss:564.36914, AverageLoss: 1426.53308, Run Time:0.327
[2019-09-18 08:40:22,397 INFO] -Training-Epoch:0(9.63406%), Global Step:130, Learning Rate:0.000100, Grad Norm:272.66812, Loss:600.37439, AverageLoss: 1360.55301, Run Time:0.332
[2019-09-18 08:40:27,084 INFO] -Training-Epoch:0(10.38088%), Global Step:140, Learning Rate:0.000100, Grad Norm:198.14577, Loss:490.62238, AverageLoss: 1303.54412, Run Time:0.329
[2019-09-18 08:40:31,324 INFO] -Training-Epoch:0(11.12771%), Global Step:150, Learning Rate:0.000100, Grad Norm:166.43112, Loss:524.35590, AverageLoss: 1252.31299, Run Time:0.341
[2019-09-18 08:40:35,893 INFO] -Training-Epoch:0(11.87453%), Global Step:160, Learning Rate:0.000100, Grad Norm:118.93724, Loss:469.45450, AverageLoss: 1206.68544, Run Time:0.297
[2019-09-18 08:40:40,764 INFO] -Training-Epoch:0(12.62136%), Global Step:170, Learning Rate:0.000100, Grad Norm:144.22699, Loss:538.69562, AverageLoss: 1166.18732, Run Time:0.336
[2019-09-18 08:40:45,523 INFO] -Training-Epoch:0(13.36819%), Global Step:180, Learning Rate:0.000100, Grad Norm:141.24054, Loss:596.35797, AverageLoss: 1131.14886, Run Time:0.325
[2019-09-18 08:40:50,039 INFO] -Training-Epoch:0(14.11501%), Global Step:190, Learning Rate:0.000100, Grad Norm:92.10086, Loss:500.82581, AverageLoss: 1098.48681, Run Time:0.348
[2019-09-18 08:40:54,581 INFO] -Training-Epoch:0(14.86184%), Global Step:200, Learning Rate:0.000100, Grad Norm:89.41254, Loss:540.96753, AverageLoss: 1070.00359, Run Time:0.335
[2019-09-18 08:40:59,234 INFO] -Training-Epoch:0(15.60866%), Global Step:210, Learning Rate:0.000100, Grad Norm:59.90015, Loss:536.69733, AverageLoss: 1042.80802, Run Time:0.338
[2019-09-18 08:41:03,723 INFO] -Training-Epoch:0(16.35549%), Global Step:220, Learning Rate:0.000100, Grad Norm:81.32962, Loss:486.03302, AverageLoss: 1017.28475, Run Time:0.333
[2019-09-18 08:41:08,274 INFO] -Training-Epoch:0(17.10232%), Global Step:230, Learning Rate:0.000100, Grad Norm:55.04326, Loss:481.44449, AverageLoss: 994.96279, Run Time:0.314
[2019-09-18 08:41:12,846 INFO] -Training-Epoch:0(17.84914%), Global Step:240, Learning Rate:0.000100, Grad Norm:112.19746, Loss:579.17487, AverageLoss: 974.08431, Run Time:0.331
[2019-09-18 08:41:17,463 INFO] -Training-Epoch:0(18.59597%), Global Step:250, Learning Rate:0.000100, Grad Norm:92.26883, Loss:503.59415, AverageLoss: 955.01898, Run Time:0.316
[2019-09-18 08:41:21,950 INFO] -Training-Epoch:0(19.34279%), Global Step:260, Learning Rate:0.000100, Grad Norm:44.27869, Loss:453.61475, AverageLoss: 936.66528, Run Time:0.309
[2019-09-18 08:41:26,491 INFO] -Training-Epoch:0(20.08962%), Global Step:270, Learning Rate:0.000100, Grad Norm:47.48191, Loss:481.45364, AverageLoss: 919.77434, Run Time:0.351
[2019-09-18 08:41:30,996 INFO] -Training-Epoch:0(20.83645%), Global Step:280, Learning Rate:0.000100, Grad Norm:101.38787, Loss:415.26956, AverageLoss: 904.01266, Run Time:0.317
[2019-09-18 08:41:35,293 INFO] -Training-Epoch:0(21.58327%), Global Step:290, Learning Rate:0.000100, Grad Norm:65.42917, Loss:517.69604, AverageLoss: 890.02608, Run Time:0.315
[2019-09-18 08:41:39,785 INFO] -Training-Epoch:0(22.33010%), Global Step:300, Learning Rate:0.000100, Grad Norm:97.49146, Loss:513.34790, AverageLoss: 876.72754, Run Time:0.324
[2019-09-18 08:41:44,290 INFO] -Training-Epoch:0(23.07692%), Global Step:310, Learning Rate:0.000100, Grad Norm:44.57857, Loss:473.68286, AverageLoss: 864.06273, Run Time:0.341
[2019-09-18 08:41:48,658 INFO] -Training-Epoch:0(23.82375%), Global Step:320, Learning Rate:0.000100, Grad Norm:58.10875, Loss:470.03500, AverageLoss: 852.77076, Run Time:0.343
[2019-09-18 08:41:53,633 INFO] -Training-Epoch:0(24.57058%), Global Step:330, Learning Rate:0.000100, Grad Norm:88.85567, Loss:489.98428, AverageLoss: 841.86152, Run Time:0.328
[2019-09-18 08:41:58,234 INFO] -Training-Epoch:0(25.31740%), Global Step:340, Learning Rate:0.000100, Grad Norm:58.87092, Loss:475.48215, AverageLoss: 831.61967, Run Time:0.316
[2019-09-18 08:42:02,752 INFO] -Training-Epoch:0(26.06423%), Global Step:350, Learning Rate:0.000100, Grad Norm:63.96300, Loss:482.01227, AverageLoss: 822.13626, Run Time:0.322
[2019-09-18 08:42:07,050 INFO] -Training-Epoch:0(26.81105%), Global Step:360, Learning Rate:0.000100, Grad Norm:96.15427, Loss:454.98914, AverageLoss: 812.91275, Run Time:0.344
[2019-09-18 08:42:11,912 INFO] -Training-Epoch:0(27.55788%), Global Step:370, Learning Rate:0.000100, Grad Norm:102.70373, Loss:475.79413, AverageLoss: 804.00496, Run Time:0.335
[2019-09-18 08:42:16,705 INFO] -Training-Epoch:0(28.30471%), Global Step:380, Learning Rate:0.000100, Grad Norm:64.99404, Loss:484.01709, AverageLoss: 795.70387, Run Time:0.339
[2019-09-18 08:42:20,988 INFO] -Training-Epoch:0(29.05153%), Global Step:390, Learning Rate:0.000100, Grad Norm:131.95998, Loss:467.82068, AverageLoss: 787.18540, Run Time:0.318
[2019-09-18 08:42:25,664 INFO] -Training-Epoch:0(29.79836%), Global Step:400, Learning Rate:0.000100, Grad Norm:51.10976, Loss:424.37622, AverageLoss: 779.52720, Run Time:0.302
[2019-09-18 08:42:30,305 INFO] -Training-Epoch:0(30.54518%), Global Step:410, Learning Rate:0.000100, Grad Norm:61.00007, Loss:410.39307, AverageLoss: 772.10497, Run Time:0.329
[2019-09-18 08:42:34,878 INFO] -Training-Epoch:0(31.29201%), Global Step:420, Learning Rate:0.000100, Grad Norm:81.70964, Loss:454.19385, AverageLoss: 764.89826, Run Time:0.324
[2019-09-18 08:42:39,391 INFO] -Training-Epoch:0(32.03883%), Global Step:430, Learning Rate:0.000100, Grad Norm:73.33310, Loss:477.11594, AverageLoss: 758.57496, Run Time:0.333
[2019-09-18 08:42:43,983 INFO] -Training-Epoch:0(32.78566%), Global Step:440, Learning Rate:0.000100, Grad Norm:152.58907, Loss:496.22165, AverageLoss: 752.13408, Run Time:0.315
[2019-09-18 08:42:48,359 INFO] -Training-Epoch:0(33.53249%), Global Step:450, Learning Rate:0.000100, Grad Norm:83.72517, Loss:416.41965, AverageLoss: 745.50874, Run Time:0.305
[2019-09-18 08:42:53,052 INFO] -Training-Epoch:0(34.27931%), Global Step:460, Learning Rate:0.000100, Grad Norm:70.31512, Loss:475.70142, AverageLoss: 739.11707, Run Time:0.331
[2019-09-18 08:42:57,453 INFO] -Training-Epoch:0(35.02614%), Global Step:470, Learning Rate:0.000100, Grad Norm:69.92332, Loss:476.08130, AverageLoss: 733.27488, Run Time:0.328
[2019-09-18 08:43:01,955 INFO] -Training-Epoch:0(35.77296%), Global Step:480, Learning Rate:0.000100, Grad Norm:82.28268, Loss:472.60202, AverageLoss: 727.67181, Run Time:0.334
[2019-09-18 08:43:06,637 INFO] -Training-Epoch:0(36.51979%), Global Step:490, Learning Rate:0.000100, Grad Norm:104.02295, Loss:447.22784, AverageLoss: 722.25857, Run Time:0.320
[2019-09-18 08:43:11,250 INFO] -Training-Epoch:0(37.26662%), Global Step:500, Learning Rate:0.000100, Grad Norm:65.20397, Loss:479.21167, AverageLoss: 716.58497, Run Time:0.320
[2019-09-18 08:43:15,672 INFO] -Training-Epoch:0(38.01344%), Global Step:510, Learning Rate:0.000100, Grad Norm:76.74235, Loss:454.68906, AverageLoss: 711.38653, Run Time:0.307
[2019-09-18 08:43:20,201 INFO] -Training-Epoch:0(38.76027%), Global Step:520, Learning Rate:0.000100, Grad Norm:142.31357, Loss:443.40500, AverageLoss: 706.57066, Run Time:0.317
[2019-09-18 08:43:24,765 INFO] -Training-Epoch:0(39.50709%), Global Step:530, Learning Rate:0.000100, Grad Norm:103.02009, Loss:431.47437, AverageLoss: 701.28503, Run Time:0.342
[2019-09-18 08:43:29,249 INFO] -Training-Epoch:0(40.25392%), Global Step:540, Learning Rate:0.000100, Grad Norm:83.62134, Loss:440.77344, AverageLoss: 696.45177, Run Time:0.334
[2019-09-18 08:43:33,366 INFO] -Training-Epoch:0(41.00075%), Global Step:550, Learning Rate:0.000100, Grad Norm:61.86228, Loss:431.68051, AverageLoss: 691.65815, Run Time:0.316
[2019-09-18 08:43:37,846 INFO] -Training-Epoch:0(41.74757%), Global Step:560, Learning Rate:0.000100, Grad Norm:130.32932, Loss:425.48291, AverageLoss: 687.09143, Run Time:0.297
[2019-09-18 08:43:42,220 INFO] -Training-Epoch:0(42.49440%), Global Step:570, Learning Rate:0.000100, Grad Norm:67.95921, Loss:468.71274, AverageLoss: 682.96585, Run Time:0.343
[2019-09-18 08:43:46,520 INFO] -Training-Epoch:0(43.24122%), Global Step:580, Learning Rate:0.000100, Grad Norm:81.59971, Loss:455.91464, AverageLoss: 678.88724, Run Time:0.338
[2019-09-18 08:43:51,014 INFO] -Training-Epoch:0(43.98805%), Global Step:590, Learning Rate:0.000100, Grad Norm:68.94891, Loss:390.95767, AverageLoss: 675.03506, Run Time:0.320
[2019-09-18 08:43:55,380 INFO] -Training-Epoch:0(44.73488%), Global Step:600, Learning Rate:0.000100, Grad Norm:49.76729, Loss:381.08511, AverageLoss: 670.68861, Run Time:0.313
[2019-09-18 08:43:59,967 INFO] -Training-Epoch:0(45.48170%), Global Step:610, Learning Rate:0.000100, Grad Norm:61.59079, Loss:377.10440, AverageLoss: 666.63592, Run Time:0.315
[2019-09-18 08:44:04,570 INFO] -Training-Epoch:0(46.22853%), Global Step:620, Learning Rate:0.000100, Grad Norm:68.48732, Loss:422.87329, AverageLoss: 662.69796, Run Time:0.332
[2019-09-18 08:44:09,114 INFO] -Training-Epoch:0(46.97535%), Global Step:630, Learning Rate:0.000100, Grad Norm:90.58337, Loss:418.91202, AverageLoss: 658.90876, Run Time:0.315
[2019-09-18 08:44:13,302 INFO] -Training-Epoch:0(47.72218%), Global Step:640, Learning Rate:0.000100, Grad Norm:53.07059, Loss:386.51968, AverageLoss: 654.91909, Run Time:0.331
[2019-09-18 08:44:17,818 INFO] -Training-Epoch:0(48.46901%), Global Step:650, Learning Rate:0.000100, Grad Norm:100.20997, Loss:456.12613, AverageLoss: 651.26473, Run Time:0.314
[2019-09-18 08:44:22,240 INFO] -Training-Epoch:0(49.21583%), Global Step:660, Learning Rate:0.000100, Grad Norm:108.60366, Loss:475.07227, AverageLoss: 647.66473, Run Time:0.328
[2019-09-18 08:44:26,556 INFO] -Training-Epoch:0(49.96266%), Global Step:670, Learning Rate:0.000100, Grad Norm:76.11323, Loss:431.21106, AverageLoss: 644.18501, Run Time:0.319
[2019-09-18 08:44:31,025 INFO] -Training-Epoch:0(50.70948%), Global Step:680, Learning Rate:0.000100, Grad Norm:56.53779, Loss:422.34415, AverageLoss: 640.91579, Run Time:0.335
[2019-09-18 08:44:35,601 INFO] -Training-Epoch:0(51.45631%), Global Step:690, Learning Rate:0.000100, Grad Norm:91.84245, Loss:419.72665, AverageLoss: 637.56612, Run Time:0.309
[2019-09-18 08:44:40,044 INFO] -Training-Epoch:0(52.20314%), Global Step:700, Learning Rate:0.000100, Grad Norm:77.47472, Loss:405.41891, AverageLoss: 634.12284, Run Time:0.326
[2019-09-18 08:44:44,642 INFO] -Training-Epoch:0(52.94996%), Global Step:710, Learning Rate:0.000100, Grad Norm:77.46426, Loss:377.30841, AverageLoss: 631.15144, Run Time:0.317
[2019-09-18 08:44:49,166 INFO] -Training-Epoch:0(53.69679%), Global Step:720, Learning Rate:0.000100, Grad Norm:73.63041, Loss:291.35843, AverageLoss: 627.75356, Run Time:0.318
[2019-09-18 08:44:53,475 INFO] -Training-Epoch:0(54.44361%), Global Step:730, Learning Rate:0.000100, Grad Norm:57.33495, Loss:419.56277, AverageLoss: 624.79042, Run Time:0.314
[2019-09-18 08:44:58,154 INFO] -Training-Epoch:0(55.19044%), Global Step:740, Learning Rate:0.000100, Grad Norm:53.59140, Loss:360.77042, AverageLoss: 621.89148, Run Time:0.336
[2019-09-18 08:45:02,913 INFO] -Training-Epoch:0(55.93727%), Global Step:750, Learning Rate:0.000100, Grad Norm:61.30634, Loss:395.42093, AverageLoss: 619.18928, Run Time:0.343
[2019-09-18 08:45:07,739 INFO] -Training-Epoch:0(56.68409%), Global Step:760, Learning Rate:0.000100, Grad Norm:55.99864, Loss:391.75702, AverageLoss: 616.11851, Run Time:0.310
[2019-09-18 08:45:12,259 INFO] -Training-Epoch:0(57.43092%), Global Step:770, Learning Rate:0.000100, Grad Norm:77.59277, Loss:377.41376, AverageLoss: 613.19743, Run Time:0.295
[2019-09-18 08:45:16,924 INFO] -Training-Epoch:0(58.17774%), Global Step:780, Learning Rate:0.000100, Grad Norm:102.12183, Loss:409.80655, AverageLoss: 610.50893, Run Time:0.306
[2019-09-18 08:45:21,676 INFO] -Training-Epoch:0(58.92457%), Global Step:790, Learning Rate:0.000100, Grad Norm:98.64608, Loss:378.52423, AverageLoss: 607.93654, Run Time:0.320
[2019-09-18 08:45:26,286 INFO] -Training-Epoch:0(59.67140%), Global Step:800, Learning Rate:0.000100, Grad Norm:55.91024, Loss:434.32343, AverageLoss: 605.38180, Run Time:0.333
[2019-09-18 08:45:30,745 INFO] -Training-Epoch:0(60.41822%), Global Step:810, Learning Rate:0.000100, Grad Norm:76.38253, Loss:408.15540, AverageLoss: 602.62826, Run Time:0.315
[2019-09-18 08:45:35,405 INFO] -Training-Epoch:0(61.16505%), Global Step:820, Learning Rate:0.000100, Grad Norm:53.50547, Loss:426.92923, AverageLoss: 600.05133, Run Time:0.299
[2019-09-18 08:45:39,887 INFO] -Training-Epoch:0(61.91187%), Global Step:830, Learning Rate:0.000100, Grad Norm:57.44073, Loss:360.45389, AverageLoss: 597.56355, Run Time:0.307
[2019-09-18 08:45:44,258 INFO] -Training-Epoch:0(62.65870%), Global Step:840, Learning Rate:0.000100, Grad Norm:88.86703, Loss:357.12186, AverageLoss: 594.94305, Run Time:0.310
[2019-09-18 08:45:49,107 INFO] -Training-Epoch:0(63.40553%), Global Step:850, Learning Rate:0.000100, Grad Norm:58.43098, Loss:423.01498, AverageLoss: 592.46446, Run Time:0.302
[2019-09-18 08:45:53,409 INFO] -Training-Epoch:0(64.15235%), Global Step:860, Learning Rate:0.000100, Grad Norm:83.15230, Loss:412.45074, AverageLoss: 590.15950, Run Time:0.331
[2019-09-18 08:45:57,964 INFO] -Training-Epoch:0(64.89918%), Global Step:870, Learning Rate:0.000100, Grad Norm:95.47193, Loss:383.87573, AverageLoss: 587.96270, Run Time:0.298
[2019-09-18 08:46:02,293 INFO] -Training-Epoch:0(65.64600%), Global Step:880, Learning Rate:0.000100, Grad Norm:66.86855, Loss:360.46750, AverageLoss: 585.60813, Run Time:0.319
[2019-09-18 08:46:06,767 INFO] -Training-Epoch:0(66.39283%), Global Step:890, Learning Rate:0.000100, Grad Norm:79.80730, Loss:408.28458, AverageLoss: 583.53502, Run Time:0.327
[2019-09-18 08:46:11,372 INFO] -Training-Epoch:0(67.13966%), Global Step:900, Learning Rate:0.000100, Grad Norm:64.76124, Loss:397.27997, AverageLoss: 581.43412, Run Time:0.307
[2019-09-18 08:46:15,935 INFO] -Training-Epoch:0(67.88648%), Global Step:910, Learning Rate:0.000100, Grad Norm:70.73708, Loss:398.68823, AverageLoss: 579.29381, Run Time:0.319
[2019-09-18 08:46:20,746 INFO] -Training-Epoch:0(68.63331%), Global Step:920, Learning Rate:0.000100, Grad Norm:70.37544, Loss:379.32672, AverageLoss: 577.28084, Run Time:0.339
[2019-09-18 08:46:25,213 INFO] -Training-Epoch:0(69.38013%), Global Step:930, Learning Rate:0.000100, Grad Norm:72.16704, Loss:406.34280, AverageLoss: 575.11374, Run Time:0.322
[2019-09-18 08:46:29,741 INFO] -Training-Epoch:0(70.12696%), Global Step:940, Learning Rate:0.000100, Grad Norm:49.65466, Loss:337.23376, AverageLoss: 573.01871, Run Time:0.307
[2019-09-18 08:46:34,356 INFO] -Training-Epoch:0(70.87379%), Global Step:950, Learning Rate:0.000100, Grad Norm:62.17203, Loss:342.32925, AverageLoss: 570.87255, Run Time:0.336
[2019-09-18 08:46:38,775 INFO] -Training-Epoch:0(71.62061%), Global Step:960, Learning Rate:0.000100, Grad Norm:57.62429, Loss:363.31384, AverageLoss: 568.82202, Run Time:0.328
[2019-09-18 08:46:43,366 INFO] -Training-Epoch:0(72.36744%), Global Step:970, Learning Rate:0.000100, Grad Norm:46.79613, Loss:369.54599, AverageLoss: 566.93447, Run Time:0.327
[2019-09-18 08:46:47,719 INFO] -Training-Epoch:0(73.11426%), Global Step:980, Learning Rate:0.000100, Grad Norm:95.31454, Loss:412.02182, AverageLoss: 565.04243, Run Time:0.317
[2019-09-18 08:46:51,998 INFO] -Training-Epoch:0(73.86109%), Global Step:990, Learning Rate:0.000100, Grad Norm:53.50904, Loss:408.50195, AverageLoss: 563.31268, Run Time:0.306
[2019-09-18 08:46:56,070 INFO] -Training-Epoch:0(74.60792%), Global Step:1000, Learning Rate:0.000100, Grad Norm:48.52711, Loss:366.22705, AverageLoss: 561.38894, Run Time:0.305
[2019-09-18 08:47:00,629 INFO] -Training-Epoch:0(75.35474%), Global Step:1010, Learning Rate:0.000100, Grad Norm:61.57121, Loss:378.92349, AverageLoss: 559.47327, Run Time:0.338
[2019-09-18 08:47:05,154 INFO] -Training-Epoch:0(76.10157%), Global Step:1020, Learning Rate:0.000100, Grad Norm:86.53558, Loss:331.91342, AverageLoss: 557.40953, Run Time:0.337
[2019-09-18 08:47:09,625 INFO] -Training-Epoch:0(76.84839%), Global Step:1030, Learning Rate:0.000100, Grad Norm:79.87672, Loss:335.05203, AverageLoss: 555.70171, Run Time:0.318
[2019-09-18 08:47:13,881 INFO] -Training-Epoch:0(77.59522%), Global Step:1040, Learning Rate:0.000100, Grad Norm:71.51655, Loss:368.67206, AverageLoss: 553.99498, Run Time:0.327
[2019-09-18 08:47:18,555 INFO] -Training-Epoch:0(78.34205%), Global Step:1050, Learning Rate:0.000100, Grad Norm:58.10644, Loss:389.81348, AverageLoss: 552.35740, Run Time:0.330
[2019-09-18 08:47:22,988 INFO] -Training-Epoch:0(79.08887%), Global Step:1060, Learning Rate:0.000100, Grad Norm:60.05577, Loss:362.13480, AverageLoss: 550.55709, Run Time:0.311
[2019-09-18 08:47:27,479 INFO] -Training-Epoch:0(79.83570%), Global Step:1070, Learning Rate:0.000100, Grad Norm:79.38824, Loss:386.48154, AverageLoss: 548.70890, Run Time:0.329
[2019-09-18 08:47:32,088 INFO] -Training-Epoch:0(80.58252%), Global Step:1080, Learning Rate:0.000100, Grad Norm:50.00191, Loss:361.32687, AverageLoss: 547.01445, Run Time:0.321
[2019-09-18 08:47:36,838 INFO] -Training-Epoch:0(81.32935%), Global Step:1090, Learning Rate:0.000100, Grad Norm:60.30861, Loss:387.05560, AverageLoss: 545.49346, Run Time:0.320
[2019-09-18 08:47:41,424 INFO] -Training-Epoch:0(82.07618%), Global Step:1100, Learning Rate:0.000100, Grad Norm:67.97656, Loss:438.40109, AverageLoss: 543.99610, Run Time:0.330
[2019-09-18 08:47:46,137 INFO] -Training-Epoch:0(82.82300%), Global Step:1110, Learning Rate:0.000100, Grad Norm:58.78903, Loss:320.48962, AverageLoss: 542.35070, Run Time:0.317
[2019-09-18 08:47:50,818 INFO] -Training-Epoch:0(83.56983%), Global Step:1120, Learning Rate:0.000100, Grad Norm:91.13814, Loss:340.73618, AverageLoss: 540.78221, Run Time:0.328
[2019-09-18 08:47:55,268 INFO] -Training-Epoch:0(84.31665%), Global Step:1130, Learning Rate:0.000100, Grad Norm:57.71730, Loss:440.35049, AverageLoss: 539.33767, Run Time:0.325
[2019-09-18 08:47:59,793 INFO] -Training-Epoch:0(85.06348%), Global Step:1140, Learning Rate:0.000100, Grad Norm:53.03426, Loss:405.89758, AverageLoss: 537.83116, Run Time:0.323
[2019-09-18 08:48:04,608 INFO] -Training-Epoch:0(85.81031%), Global Step:1150, Learning Rate:0.000100, Grad Norm:51.83998, Loss:307.84924, AverageLoss: 536.34682, Run Time:0.309
[2019-09-18 08:48:09,432 INFO] -Training-Epoch:0(86.55713%), Global Step:1160, Learning Rate:0.000100, Grad Norm:58.62188, Loss:359.86087, AverageLoss: 534.87272, Run Time:0.329
[2019-09-18 08:48:13,905 INFO] -Training-Epoch:0(87.30396%), Global Step:1170, Learning Rate:0.000100, Grad Norm:99.32782, Loss:383.34668, AverageLoss: 533.45132, Run Time:0.313
[2019-09-18 08:48:18,531 INFO] -Training-Epoch:0(88.05078%), Global Step:1180, Learning Rate:0.000100, Grad Norm:78.30759, Loss:360.68658, AverageLoss: 531.94686, Run Time:0.327
[2019-09-18 08:48:22,810 INFO] -Training-Epoch:0(88.79761%), Global Step:1190, Learning Rate:0.000100, Grad Norm:54.05598, Loss:399.08932, AverageLoss: 530.56705, Run Time:0.344
[2019-09-18 08:48:27,634 INFO] -Training-Epoch:0(89.54444%), Global Step:1200, Learning Rate:0.000100, Grad Norm:65.47483, Loss:348.13840, AverageLoss: 529.04999, Run Time:0.307
[2019-09-18 08:48:32,028 INFO] -Training-Epoch:0(90.29126%), Global Step:1210, Learning Rate:0.000100, Grad Norm:58.72521, Loss:336.51376, AverageLoss: 527.58981, Run Time:0.327
[2019-09-18 08:48:36,725 INFO] -Training-Epoch:0(91.03809%), Global Step:1220, Learning Rate:0.000100, Grad Norm:78.37882, Loss:313.09106, AverageLoss: 526.19165, Run Time:0.333
[2019-09-18 08:48:41,236 INFO] -Training-Epoch:0(91.78491%), Global Step:1230, Learning Rate:0.000100, Grad Norm:65.09593, Loss:347.78641, AverageLoss: 524.78719, Run Time:0.341
[2019-09-18 08:48:45,989 INFO] -Training-Epoch:0(92.53174%), Global Step:1240, Learning Rate:0.000100, Grad Norm:43.38807, Loss:345.33908, AverageLoss: 523.38777, Run Time:0.318
[2019-09-18 08:48:50,610 INFO] -Training-Epoch:0(93.27857%), Global Step:1250, Learning Rate:0.000100, Grad Norm:57.80871, Loss:437.84769, AverageLoss: 522.12178, Run Time:0.315
[2019-09-18 08:48:55,384 INFO] -Training-Epoch:0(94.02539%), Global Step:1260, Learning Rate:0.000100, Grad Norm:48.64845, Loss:374.14169, AverageLoss: 520.87025, Run Time:0.306
[2019-09-18 08:48:59,721 INFO] -Training-Epoch:0(94.77222%), Global Step:1270, Learning Rate:0.000100, Grad Norm:82.16091, Loss:373.51422, AverageLoss: 519.58146, Run Time:0.313
[2019-09-18 08:49:04,382 INFO] -Training-Epoch:0(95.51904%), Global Step:1280, Learning Rate:0.000100, Grad Norm:42.94859, Loss:369.49957, AverageLoss: 518.36955, Run Time:0.330
[2019-09-18 08:49:08,841 INFO] -Training-Epoch:0(96.26587%), Global Step:1290, Learning Rate:0.000100, Grad Norm:53.88337, Loss:335.65833, AverageLoss: 516.94362, Run Time:0.312
[2019-09-18 08:49:13,581 INFO] -Training-Epoch:0(97.01270%), Global Step:1300, Learning Rate:0.000100, Grad Norm:70.26364, Loss:373.05688, AverageLoss: 515.78290, Run Time:0.327
[2019-09-18 08:49:18,378 INFO] -Training-Epoch:0(97.75952%), Global Step:1310, Learning Rate:0.000100, Grad Norm:52.88046, Loss:371.03583, AverageLoss: 514.54729, Run Time:0.326
[2019-09-18 08:49:23,066 INFO] -Training-Epoch:0(98.50635%), Global Step:1320, Learning Rate:0.000100, Grad Norm:84.68352, Loss:323.07407, AverageLoss: 513.28596, Run Time:0.346
[2019-09-18 08:49:27,495 INFO] -Training-Epoch:0(99.25317%), Global Step:1330, Learning Rate:0.000100, Grad Norm:62.57191, Loss:395.30780, AverageLoss: 512.09747, Run Time:0.341
[2019-09-18 08:49:31,534 INFO] -Training-Epoch:0, Average Loss: 511.12473, Epoch Time: 440.566
[2019-09-18 08:49:32,396 INFO] -Validation-Epoch:0(0.00000%), CER: 13.69048 %
[2019-09-18 08:49:35,658 INFO] -Validation-Epoch:0(12.19512%), CER: 12.26601 %
[2019-09-18 08:49:38,704 INFO] -Validation-Epoch:0(24.39024%), CER: 13.20367 %
[2019-09-18 08:49:41,723 INFO] -Validation-Epoch:0(36.58537%), CER: 12.71701 %
[2019-09-18 08:49:44,839 INFO] -Validation-Epoch:0(48.78049%), CER: 12.84020 %
[2019-09-18 08:49:47,861 INFO] -Validation-Epoch:0(60.97561%), CER: 12.73218 %
[2019-09-18 08:49:50,800 INFO] -Validation-Epoch:0(73.17073%), CER: 12.56708 %
[2019-09-18 08:49:53,454 INFO] -Validation-Epoch:0(85.36585%), CER: 12.77752 %
[2019-09-18 08:49:56,595 INFO] -Validation-Epoch:0(97.56098%), CER: 12.71992 %
[2019-09-18 08:49:56,744 INFO] -Validation-Epoch:   0, AverageLoss:0.00000, AverageCER: 12.76548 %
[2019-09-18 08:49:56,772 INFO] Epoch 0 model has been saved.
[2019-09-18 08:50:02,484 INFO] -Training-Epoch:1(0.67214%), Global Step:10, Learning Rate:0.000100, Grad Norm:60.48057, Loss:326.43210, AverageLoss: 371.00082, Run Time:0.320
[2019-09-18 08:50:06,948 INFO] -Training-Epoch:1(1.41897%), Global Step:20, Learning Rate:0.000100, Grad Norm:72.24327, Loss:367.42776, AverageLoss: 357.45607, Run Time:0.336
[2019-09-18 08:50:11,571 INFO] -Training-Epoch:1(2.16580%), Global Step:30, Learning Rate:0.000100, Grad Norm:71.36216, Loss:372.64413, AverageLoss: 358.83285, Run Time:0.331
[2019-09-18 08:50:16,402 INFO] -Training-Epoch:1(2.91262%), Global Step:40, Learning Rate:0.000100, Grad Norm:57.61109, Loss:333.96173, AverageLoss: 356.42834, Run Time:0.340
[2019-09-18 08:50:21,067 INFO] -Training-Epoch:1(3.65945%), Global Step:50, Learning Rate:0.000100, Grad Norm:122.80128, Loss:340.30151, AverageLoss: 358.47808, Run Time:0.327
[2019-09-18 08:50:25,668 INFO] -Training-Epoch:1(4.40627%), Global Step:60, Learning Rate:0.000100, Grad Norm:56.42306, Loss:365.63486, AverageLoss: 356.41762, Run Time:0.305
[2019-09-18 08:50:30,215 INFO] -Training-Epoch:1(5.15310%), Global Step:70, Learning Rate:0.000100, Grad Norm:61.26183, Loss:359.81573, AverageLoss: 354.07136, Run Time:0.321
[2019-09-18 08:50:34,926 INFO] -Training-Epoch:1(5.89993%), Global Step:80, Learning Rate:0.000100, Grad Norm:69.41100, Loss:365.83105, AverageLoss: 354.90831, Run Time:0.336
[2019-09-18 08:50:39,268 INFO] -Training-Epoch:1(6.64675%), Global Step:90, Learning Rate:0.000100, Grad Norm:47.57224, Loss:343.40610, AverageLoss: 353.85014, Run Time:0.299
[2019-09-18 08:50:44,035 INFO] -Training-Epoch:1(7.39358%), Global Step:100, Learning Rate:0.000100, Grad Norm:56.18319, Loss:362.73825, AverageLoss: 355.39478, Run Time:0.343
[2019-09-18 08:50:48,520 INFO] -Training-Epoch:1(8.14040%), Global Step:110, Learning Rate:0.000100, Grad Norm:84.96783, Loss:317.33115, AverageLoss: 355.28335, Run Time:0.320
[2019-09-18 08:50:53,158 INFO] -Training-Epoch:1(8.88723%), Global Step:120, Learning Rate:0.000100, Grad Norm:60.61787, Loss:357.15814, AverageLoss: 353.95525, Run Time:0.327
[2019-09-18 08:50:57,954 INFO] -Training-Epoch:1(9.63406%), Global Step:130, Learning Rate:0.000100, Grad Norm:84.60336, Loss:358.14575, AverageLoss: 353.98069, Run Time:0.320
[2019-09-18 08:51:02,597 INFO] -Training-Epoch:1(10.38088%), Global Step:140, Learning Rate:0.000100, Grad Norm:54.60579, Loss:346.77142, AverageLoss: 353.47416, Run Time:0.326
[2019-09-18 08:51:07,396 INFO] -Training-Epoch:1(11.12771%), Global Step:150, Learning Rate:0.000100, Grad Norm:54.45188, Loss:296.52036, AverageLoss: 352.76726, Run Time:0.305
[2019-09-18 08:51:12,011 INFO] -Training-Epoch:1(11.87453%), Global Step:160, Learning Rate:0.000100, Grad Norm:65.33399, Loss:365.43826, AverageLoss: 352.48479, Run Time:0.330
[2019-09-18 08:51:16,557 INFO] -Training-Epoch:1(12.62136%), Global Step:170, Learning Rate:0.000100, Grad Norm:44.99538, Loss:302.44003, AverageLoss: 352.24388, Run Time:0.300
[2019-09-18 08:51:21,107 INFO] -Training-Epoch:1(13.36819%), Global Step:180, Learning Rate:0.000100, Grad Norm:48.98185, Loss:313.47809, AverageLoss: 351.09786, Run Time:0.316
[2019-09-18 08:51:25,318 INFO] -Training-Epoch:1(14.11501%), Global Step:190, Learning Rate:0.000100, Grad Norm:79.34730, Loss:386.05325, AverageLoss: 350.72298, Run Time:0.320
[2019-09-18 08:51:29,745 INFO] -Training-Epoch:1(14.86184%), Global Step:200, Learning Rate:0.000100, Grad Norm:62.84822, Loss:310.54349, AverageLoss: 350.20228, Run Time:0.320
[2019-09-18 08:51:34,296 INFO] -Training-Epoch:1(15.60866%), Global Step:210, Learning Rate:0.000100, Grad Norm:78.19838, Loss:400.19067, AverageLoss: 350.01461, Run Time:0.327
[2019-09-18 08:51:38,719 INFO] -Training-Epoch:1(16.35549%), Global Step:220, Learning Rate:0.000100, Grad Norm:65.73429, Loss:362.28064, AverageLoss: 349.66288, Run Time:0.337
[2019-09-18 08:51:43,324 INFO] -Training-Epoch:1(17.10232%), Global Step:230, Learning Rate:0.000100, Grad Norm:78.23968, Loss:371.38193, AverageLoss: 350.29650, Run Time:0.317
[2019-09-18 08:51:47,826 INFO] -Training-Epoch:1(17.84914%), Global Step:240, Learning Rate:0.000100, Grad Norm:111.60447, Loss:388.66141, AverageLoss: 349.74625, Run Time:0.332
[2019-09-18 08:51:52,310 INFO] -Training-Epoch:1(18.59597%), Global Step:250, Learning Rate:0.000100, Grad Norm:53.92690, Loss:287.12387, AverageLoss: 349.63201, Run Time:0.328
[2019-09-18 08:51:56,835 INFO] -Training-Epoch:1(19.34279%), Global Step:260, Learning Rate:0.000100, Grad Norm:97.71750, Loss:359.32742, AverageLoss: 349.12831, Run Time:0.333
[2019-09-18 08:52:01,008 INFO] -Training-Epoch:1(20.08962%), Global Step:270, Learning Rate:0.000100, Grad Norm:71.51537, Loss:386.08920, AverageLoss: 348.68017, Run Time:0.328
[2019-09-18 08:52:05,575 INFO] -Training-Epoch:1(20.83645%), Global Step:280, Learning Rate:0.000100, Grad Norm:90.60768, Loss:351.72003, AverageLoss: 348.52399, Run Time:0.319
[2019-09-18 08:52:09,960 INFO] -Training-Epoch:1(21.58327%), Global Step:290, Learning Rate:0.000100, Grad Norm:69.33806, Loss:306.81744, AverageLoss: 347.68816, Run Time:0.291
[2019-09-18 08:52:14,327 INFO] -Training-Epoch:1(22.33010%), Global Step:300, Learning Rate:0.000100, Grad Norm:61.35043, Loss:302.78726, AverageLoss: 346.99381, Run Time:0.331
[2019-09-18 08:52:19,018 INFO] -Training-Epoch:1(23.07692%), Global Step:310, Learning Rate:0.000100, Grad Norm:120.55478, Loss:336.96155, AverageLoss: 346.83299, Run Time:0.313
[2019-09-18 08:52:23,351 INFO] -Training-Epoch:1(23.82375%), Global Step:320, Learning Rate:0.000100, Grad Norm:60.92053, Loss:339.48895, AverageLoss: 346.29846, Run Time:0.318
[2019-09-18 08:52:27,760 INFO] -Training-Epoch:1(24.57058%), Global Step:330, Learning Rate:0.000100, Grad Norm:91.72940, Loss:363.52795, AverageLoss: 345.63422, Run Time:0.313
[2019-09-18 08:52:32,228 INFO] -Training-Epoch:1(25.31740%), Global Step:340, Learning Rate:0.000100, Grad Norm:53.50729, Loss:374.19971, AverageLoss: 345.66742, Run Time:0.329
[2019-09-18 08:52:36,701 INFO] -Training-Epoch:1(26.06423%), Global Step:350, Learning Rate:0.000100, Grad Norm:56.05769, Loss:331.52423, AverageLoss: 345.04578, Run Time:0.310
[2019-09-18 08:52:40,886 INFO] -Training-Epoch:1(26.81105%), Global Step:360, Learning Rate:0.000100, Grad Norm:68.80070, Loss:289.12997, AverageLoss: 344.55279, Run Time:0.337
[2019-09-18 08:52:45,453 INFO] -Training-Epoch:1(27.55788%), Global Step:370, Learning Rate:0.000100, Grad Norm:50.98137, Loss:337.76035, AverageLoss: 344.12179, Run Time:0.304
[2019-09-18 08:52:50,146 INFO] -Training-Epoch:1(28.30471%), Global Step:380, Learning Rate:0.000100, Grad Norm:56.52798, Loss:302.55820, AverageLoss: 344.23868, Run Time:0.315
[2019-09-18 08:52:54,614 INFO] -Training-Epoch:1(29.05153%), Global Step:390, Learning Rate:0.000100, Grad Norm:78.92044, Loss:367.33533, AverageLoss: 344.22177, Run Time:0.319
[2019-09-18 08:52:59,206 INFO] -Training-Epoch:1(29.79836%), Global Step:400, Learning Rate:0.000100, Grad Norm:68.42119, Loss:248.16685, AverageLoss: 343.86307, Run Time:0.321
[2019-09-18 08:53:03,941 INFO] -Training-Epoch:1(30.54518%), Global Step:410, Learning Rate:0.000100, Grad Norm:64.75284, Loss:332.38150, AverageLoss: 343.64084, Run Time:0.319
[2019-09-18 08:53:08,735 INFO] -Training-Epoch:1(31.29201%), Global Step:420, Learning Rate:0.000100, Grad Norm:73.16285, Loss:340.07693, AverageLoss: 343.13923, Run Time:0.331
[2019-09-18 08:53:13,404 INFO] -Training-Epoch:1(32.03883%), Global Step:430, Learning Rate:0.000100, Grad Norm:70.83857, Loss:333.26254, AverageLoss: 343.04617, Run Time:0.337
[2019-09-18 08:53:18,035 INFO] -Training-Epoch:1(32.78566%), Global Step:440, Learning Rate:0.000100, Grad Norm:79.62881, Loss:358.88132, AverageLoss: 343.14021, Run Time:0.316
[2019-09-18 08:53:22,353 INFO] -Training-Epoch:1(33.53249%), Global Step:450, Learning Rate:0.000100, Grad Norm:60.74484, Loss:357.08368, AverageLoss: 343.04960, Run Time:0.325
[2019-09-18 08:53:26,510 INFO] -Training-Epoch:1(34.27931%), Global Step:460, Learning Rate:0.000100, Grad Norm:105.24989, Loss:376.06558, AverageLoss: 342.77514, Run Time:0.316
[2019-09-18 08:53:30,868 INFO] -Training-Epoch:1(35.02614%), Global Step:470, Learning Rate:0.000100, Grad Norm:93.54659, Loss:343.65234, AverageLoss: 342.43513, Run Time:0.317
[2019-09-18 08:53:35,138 INFO] -Training-Epoch:1(35.77296%), Global Step:480, Learning Rate:0.000100, Grad Norm:60.42510, Loss:253.88728, AverageLoss: 342.08719, Run Time:0.315
[2019-09-18 08:53:39,584 INFO] -Training-Epoch:1(36.51979%), Global Step:490, Learning Rate:0.000100, Grad Norm:99.33985, Loss:395.58914, AverageLoss: 341.83323, Run Time:0.330
[2019-09-18 08:53:44,342 INFO] -Training-Epoch:1(37.26662%), Global Step:500, Learning Rate:0.000100, Grad Norm:93.29243, Loss:331.38376, AverageLoss: 341.52124, Run Time:0.321
[2019-09-18 08:53:48,565 INFO] -Training-Epoch:1(38.01344%), Global Step:510, Learning Rate:0.000100, Grad Norm:75.70783, Loss:368.69308, AverageLoss: 341.41405, Run Time:0.325
[2019-09-18 08:53:53,065 INFO] -Training-Epoch:1(38.76027%), Global Step:520, Learning Rate:0.000100, Grad Norm:64.49788, Loss:321.30383, AverageLoss: 341.03523, Run Time:0.320
[2019-09-18 08:53:57,590 INFO] -Training-Epoch:1(39.50709%), Global Step:530, Learning Rate:0.000100, Grad Norm:56.17022, Loss:312.30029, AverageLoss: 340.86385, Run Time:0.316
[2019-09-18 08:54:02,165 INFO] -Training-Epoch:1(40.25392%), Global Step:540, Learning Rate:0.000100, Grad Norm:65.77659, Loss:303.36798, AverageLoss: 340.37739, Run Time:0.330
[2019-09-18 08:54:06,713 INFO] -Training-Epoch:1(41.00075%), Global Step:550, Learning Rate:0.000100, Grad Norm:104.72408, Loss:322.62479, AverageLoss: 340.19398, Run Time:0.315
[2019-09-18 08:54:11,061 INFO] -Training-Epoch:1(41.74757%), Global Step:560, Learning Rate:0.000100, Grad Norm:64.75101, Loss:359.71292, AverageLoss: 340.31219, Run Time:0.338
[2019-09-18 08:54:15,486 INFO] -Training-Epoch:1(42.49440%), Global Step:570, Learning Rate:0.000100, Grad Norm:86.64102, Loss:301.84204, AverageLoss: 339.72554, Run Time:0.322
[2019-09-18 08:54:19,603 INFO] -Training-Epoch:1(43.24122%), Global Step:580, Learning Rate:0.000100, Grad Norm:70.12466, Loss:346.19229, AverageLoss: 339.75328, Run Time:0.329
[2019-09-18 08:54:24,444 INFO] -Training-Epoch:1(43.98805%), Global Step:590, Learning Rate:0.000100, Grad Norm:65.15682, Loss:356.01929, AverageLoss: 339.78676, Run Time:0.302
[2019-09-18 08:54:29,063 INFO] -Training-Epoch:1(44.73488%), Global Step:600, Learning Rate:0.000100, Grad Norm:90.40752, Loss:361.75998, AverageLoss: 339.65837, Run Time:0.339
[2019-09-18 08:54:33,484 INFO] -Training-Epoch:1(45.48170%), Global Step:610, Learning Rate:0.000100, Grad Norm:76.21403, Loss:334.58627, AverageLoss: 339.14094, Run Time:0.299
[2019-09-18 08:54:38,058 INFO] -Training-Epoch:1(46.22853%), Global Step:620, Learning Rate:0.000100, Grad Norm:98.76569, Loss:343.60730, AverageLoss: 338.87745, Run Time:0.331
[2019-09-18 08:54:42,578 INFO] -Training-Epoch:1(46.97535%), Global Step:630, Learning Rate:0.000100, Grad Norm:80.39545, Loss:302.57764, AverageLoss: 338.61365, Run Time:0.308
